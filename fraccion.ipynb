{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "666573a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas disponibles después del preprocesamiento:\n",
      "Index(['aveOralM', 'Gender', 'Age', 'Ethnicity', 'T_atm', 'Humidity',\n",
      "       'Cosmetics', 'Distance', 'Max1R13', 'Max1L13', 'T_Max', 'TF_HCC'],\n",
      "      dtype='object')\n",
      "\n",
      "Primeras filas del DataFrame procesado:\n",
      "   aveOralM  Gender    Age                  Ethnicity  T_atm  Humidity  \\\n",
      "0     36.59    Male  41-50                      White   24.0      28.0   \n",
      "1     37.19  Female  31-40  Black or African-American   24.0      26.0   \n",
      "2     37.34  Female  21-30                      White   24.0      26.0   \n",
      "3     37.09  Female  21-30  Black or African-American   24.0      27.0   \n",
      "4     37.04    Male  18-20                      White   24.0      27.0   \n",
      "\n",
      "   Cosmetics  Distance  Max1R13  Max1L13    T_Max   TF_HCC  \n",
      "0        NaN       0.8  35.0300  35.3775  35.6925  33.5775  \n",
      "1        NaN       0.8  34.5500  34.5200  35.1750  34.0325  \n",
      "2        NaN       0.8  35.6525  35.5175  35.9125  34.9000  \n",
      "3        NaN       0.8  35.2225  35.6125  35.7200  34.4400  \n",
      "4        NaN       0.8  35.5450  35.6650  35.8950  35.0900  \n",
      "\n",
      "Información del DataFrame procesado:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1020 entries, 0 to 1019\n",
      "Data columns (total 12 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   aveOralM   1020 non-null   float64\n",
      " 1   Gender     1020 non-null   object \n",
      " 2   Age        1020 non-null   object \n",
      " 3   Ethnicity  1020 non-null   object \n",
      " 4   T_atm      1020 non-null   float64\n",
      " 5   Humidity   1020 non-null   float64\n",
      " 6   Cosmetics  991 non-null    float64\n",
      " 7   Distance   1018 non-null   float64\n",
      " 8   Max1R13    1020 non-null   float64\n",
      " 9   Max1L13    1020 non-null   float64\n",
      " 10  T_Max      1020 non-null   float64\n",
      " 11  TF_HCC     1020 non-null   float64\n",
      "dtypes: float64(9), object(3)\n",
      "memory usage: 95.8+ KB\n",
      "\n",
      "NaNs antes de eliminarlos de model_df (columnas seleccionadas):\n",
      "aveOralM    0\n",
      "Max1R13     0\n",
      "Max1L13     0\n",
      "T_Max       0\n",
      "TF_HCC      0\n",
      "dtype: int64\n",
      "\n",
      "Forma del DataFrame después de eliminar NaNs: (1020, 5)\n",
      "\n",
      "Forma de X_train: (816, 4), X_test: (204, 4)\n",
      "\n",
      "--- Evaluación del Modelo (Scikit-learn) ---\n",
      "Coeficientes: [ 0.01442377 -0.01422809  0.91454547 -0.021138  ]\n",
      "Intercepto: 4.756965023870862\n",
      "MSE (entrenamiento): 0.0749\n",
      "R² (entrenamiento): 0.7243\n",
      "MSE (prueba): 0.0615\n",
      "R² (prueba): 0.7080\n",
      "\n",
      "--- Resumen del Modelo (Statsmodels OLS) ---\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               aveOralM   R-squared:                       0.724\n",
      "Model:                            OLS   Adj. R-squared:                  0.723\n",
      "Method:                 Least Squares   F-statistic:                     532.5\n",
      "Date:                Sat, 07 Jun 2025   Prob (F-statistic):          3.93e-225\n",
      "Time:                        18:52:36   Log-Likelihood:                -100.32\n",
      "No. Observations:                 816   AIC:                             210.6\n",
      "Df Residuals:                     811   BIC:                             234.2\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          4.7570      0.712      6.677      0.000       3.359       6.155\n",
      "Max1R13        0.0144      0.044      0.327      0.743      -0.072       0.101\n",
      "Max1L13       -0.0142      0.044     -0.322      0.748      -0.101       0.073\n",
      "T_Max          0.9145      0.044     20.901      0.000       0.829       1.000\n",
      "TF_HCC        -0.0211      0.019     -1.086      0.278      -0.059       0.017\n",
      "==============================================================================\n",
      "Omnibus:                       24.118   Durbin-Watson:                   1.883\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               39.106\n",
      "Skew:                           0.236   Prob(JB):                     3.22e-09\n",
      "Kurtosis:                       3.963   Cond. No.                     5.26e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 5.26e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 1. Cargar y Preprocesar Datos (código que proporcionaste)\n",
    "# Asegúrate de que la ruta al archivo CSV sea correcta.\n",
    "try:\n",
    "    # Reemplaza \"path/to/your/Proyecto 2/FLIR_groups1and2.csv\" con la ruta correcta\n",
    "    file_path = \"archivos/FLIR_groups1and2.csv\" # Asumiendo que el archivo está en el mismo directorio o proporciona la ruta completa\n",
    "    df = (pd.read_csv(file_path, sep=\";\" , header=2))[['Max1R13_1', 'Max1R13_2', 'Max1R13_3', 'Max1R13_4',\n",
    "                                                      'Max1L13_1', 'Max1L13_2', 'Max1L13_3', 'Max1L13_4',\n",
    "                                                      'T_Max1', 'T_Max2', 'T_Max3', 'T_Max4',\n",
    "                                                      'T_FHCC1', 'T_FHCC2', 'T_FHCC3', 'T_FHCC4',\n",
    "                                                      'aveOralM', 'Gender', 'Age', 'Ethnicity', 'T_atm', 'Humidity', 'Cosmetics','Distance']].copy()\n",
    "\n",
    "    df['Max1R13'] = df[['Max1R13_1', 'Max1R13_2', 'Max1R13_3', 'Max1R13_4']].mean(axis=1, skipna=True).astype(float)\n",
    "    df.drop(columns=['Max1R13_1', 'Max1R13_2', 'Max1R13_3', 'Max1R13_4'],inplace=True)\n",
    "\n",
    "    df['Max1L13'] = df[['Max1L13_1', 'Max1L13_2', 'Max1L13_3', 'Max1L13_4']].mean(axis=1, skipna=True).astype(float)\n",
    "    df.drop(columns=['Max1L13_1', 'Max1L13_2', 'Max1L13_3', 'Max1L13_4'],inplace=True)\n",
    "\n",
    "    df['T_Max'] = df[['T_Max1', 'T_Max2', 'T_Max3', 'T_Max4']].mean(axis=1, skipna=True).astype(float)\n",
    "    df.drop(columns=['T_Max1', 'T_Max2', 'T_Max3', 'T_Max4'],inplace=True)\n",
    "\n",
    "    df['TF_HCC'] = df[['T_FHCC1', 'T_FHCC2', 'T_FHCC3', 'T_FHCC4']].mean(axis=1, skipna=True).astype(float)\n",
    "    df.drop(columns=['T_FHCC1', 'T_FHCC2', 'T_FHCC3', 'T_FHCC4'],inplace=True)\n",
    "\n",
    "    print(\"Columnas disponibles después del preprocesamiento:\")\n",
    "    print(df.columns)\n",
    "    print(\"\\nPrimeras filas del DataFrame procesado:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nInformación del DataFrame procesado:\")\n",
    "    df.info()\n",
    "\n",
    "    # 2. Seleccionar Variables\n",
    "    # Asumiremos que 'aveOralM' es la variable dependiente (objetivo).\n",
    "    # Seleccionaremos algunas variables numéricas y una categórica ('Gender') como predictores.\n",
    "    # Puedes ajustar esta selección según los requisitos de tu proyecto.\n",
    "    # Columnas candidatas: 'Age', 'T_atm', 'Humidity', 'Distance', 'Max1R13', 'Max1L13', 'T_Max', 'TF_HCC', 'Gender'\n",
    "    \n",
    "\n",
    "    dependent_var = 'aveOralM'\n",
    "    # Empezaremos con un conjunto de predictores. Puedes expandirlo.\n",
    "    independent_vars_numerical = ['Max1R13','Max1L13', 'T_Max', 'TF_HCC']\n",
    "\n",
    "    # Filtrar solo las columnas numéricas que existen en el df\n",
    "    independent_vars_numerical = [col for col in independent_vars_numerical if col in df.columns]\n",
    "\n",
    "    selected_columns = [dependent_var] + independent_vars_numerical\n",
    "    \n",
    "    # Crear una copia para el modelado para no alterar el df original innecesariamente\n",
    "    model_df = df[selected_columns].copy()\n",
    "\n",
    "    # 3. Manejar Datos Faltantes (en las columnas seleccionadas para el modelo)\n",
    "    # Primero, verificamos si 'aveOralM' tiene NaNs y los eliminamos, ya que es nuestro objetivo.\n",
    "    model_df.dropna(subset=[dependent_var], inplace=True)\n",
    "    # Para los predictores, podemos eliminar filas con NaNs o imputar. Aquí los eliminaremos por simplicidad.\n",
    "    # Antes de eliminar, es bueno saber cuántos NaNs hay\n",
    "    print(f\"\\nNaNs antes de eliminarlos de model_df (columnas seleccionadas):\")\n",
    "    print(model_df.isnull().sum())\n",
    "    model_df.dropna(inplace=True)\n",
    "    print(f\"\\nForma del DataFrame después de eliminar NaNs: {model_df.shape}\")\n",
    "\n",
    "    if model_df.empty:\n",
    "        print(\"\\nEl DataFrame está vacío después de eliminar NaNs. Verifica tus datos y el manejo de NaNs.\")\n",
    "        # Detener la ejecución si el DataFrame está vacío\n",
    "        exit()\n",
    "\n",
    "\n",
    "    # Definir X (predictores) e y (objetivo)\n",
    "    X = model_df.drop(columns=[dependent_var])\n",
    "    y = model_df[dependent_var]\n",
    "\n",
    "    # 5. Dividir los Datos\n",
    "    # test_size es la proporción del dataset a incluir en el split de test (ej. 0.2 = 20%)\n",
    "    # random_state asegura que la división sea la misma cada vez que corras el código (reproducibilidad)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    print(f\"\\nForma de X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
    "\n",
    "    # 6. Entrenar el Modelo (usando scikit-learn)\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train, y_train)\n",
    "\n",
    "    # 7. Evaluar el Modelo\n",
    "    y_pred_train = lr_model.predict(X_train)\n",
    "    y_pred_test = lr_model.predict(X_test)\n",
    "\n",
    "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "\n",
    "    print(f\"\\n--- Evaluación del Modelo (Scikit-learn) ---\")\n",
    "    print(f\"Coeficientes: {lr_model.coef_}\")\n",
    "    print(f\"Intercepto: {lr_model.intercept_}\")\n",
    "    print(f\"MSE (entrenamiento): {mse_train:.4f}\")\n",
    "    print(f\"R² (entrenamiento): {r2_train:.4f}\")\n",
    "    print(f\"MSE (prueba): {mse_test:.4f}\")\n",
    "    print(f\"R² (prueba): {r2_test:.4f}\")\n",
    "\n",
    "    # 8. Interpretar el Modelo (usando statsmodels para un resumen más detallado)\n",
    "    # statsmodels necesita que se añada una constante para el término del intercepto\n",
    "    X_train_sm = sm.add_constant(X_train)\n",
    "    X_test_sm = sm.add_constant(X_test) # Para consistencia, aunque no siempre se usa para predecir directamente con OLS\n",
    "\n",
    "    ols_model = sm.OLS(y_train, X_train_sm)\n",
    "    ols_results = ols_model.fit()\n",
    "\n",
    "    print(f\"\\n--- Resumen del Modelo (Statsmodels OLS) ---\")\n",
    "    print(ols_results.summary())\n",
    "    \n",
    "    # Guardar el dataframe procesado y listo para el modelado (opcional)\n",
    "    # model_df.to_csv(\"datos_procesados_para_regresion.csv\", index=False)\n",
    "    # print(\"\\nDataFrame procesado guardado en 'datos_procesados_para_regresion.csv'\")\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: El archivo CSV no se encontró en la ruta especificada: {file_path}\")\n",
    "    print(\"Por favor, asegúrate de que el archivo 'FLIR_groups1and2.csv' esté en el directorio correcto o proporciona la ruta completa.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error durante la ejecución: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
